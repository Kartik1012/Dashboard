Expanded Core Metrics (with your additions)

Notation:
Q = original question,
A = generated answer,
C = {câ‚, câ‚‚ â€¦ câ‚–} retrieved contexts,
GT = ground truth (optional).

3.1 Context Recall

Checks if necessary evidence was retrieved.

With GT â†’ verify whether key facts from GT appear in any cáµ¢.

Without GT â†’ check if answer claims are supported in any cáµ¢.

Score (0â€“1): supported_facts / total_facts.

3.2 Context Precision

Proportion of retrieved contexts that are truly relevant to Q.

Score (0â€“1): relevant_contexts / total_contexts.

3.3 Context Entity Recall ğŸ”¥ (your addition)

Measures whether entities in GT (or expected answer) are covered by retrieved contexts.

Uses NER (Named Entity Recognition) to extract entities from GT/Answer and compares with entities in contexts.

Score (0â€“1): entities_covered / total_entities.

3.4 Context Utilization (enhanced definition ğŸ”¥)

Checks how many contexts are actually used by the model in the answer.

For each cáµ¢, compute semantic overlap with A â†’ if significant, mark as "used."

Score (0â€“1): used_contexts / total_contexts.
(Previously context utilization was just attribution; now we explicitly count how many contexts contributed to A.)

3.5 Faithfulness / Groundedness

Extract claims from A â†’ check if entailed by any cáµ¢ using NLI/LLM-judge.

Penalize contradictions.

Score (0â€“1): entailed_claims / total_claims.

3.6 Answer Relevancy

Measures whether A addresses Q.

Embedding similarity + LLM rubric for topical alignment.

Score (0â€“1).

3.7 Answer Similarity / Correctness vs GT

(If GT available) Compare A with GT using semantic similarity, factual match, or BERTScore.

Score (0â€“1).

3.8 QA Correctness (based on Questionâ€“Question Similarity) ğŸ”¥

Generate a rephrased/implicit question from the answer (Qâ€²).

Measure topic similarity between original Q and generated Qâ€².

If Qâ€² diverges from Q â†’ model may be answering a different question.

Score (0â€“1): sim(Q, Qâ€²).
