from dataclasses import dataclass, field
import typing as t
import numpy as np
from ragas.dataset_schema import SingleTurnSample
from ragas.metrics.base import MetricType
from your_module_path import LLMContextPrecisionWithoutReference, Verification, QAC  # <-- adjust import paths

@dataclass
class CustomContextUtilization(LLMContextPrecisionWithoutReference):
    """
    Custom ContextUtilization that calculates score as
    (relevant contexts / total contexts) instead of average precision.
    """
    name: str = "custom_context_utilization"
    _required_columns: t.Dict[str, t.Set[str]] = field(
        default_factory=lambda: {
            MetricType.SINGLE_TURN: {"user_input", "response", "retrieved_contexts"}
        }
    )

    async def _single_turn_ascore(
        self, sample: SingleTurnSample, callbacks
    ) -> float:
        row = sample.to_dict()
        user_input, retrieved_contexts, response = self._get_row_attributes(row)

        verdicts = []
        for context in retrieved_contexts:
            # LLM ko prompt bhej kar verdict lena
            results: t.List[Verification] = await self.context_precision_prompt.generate_multiple(
                data=QAC(
                    question=user_input,
                    context=context,
                    answer=response,
                ),
                llm=self.llm,
                callbacks=callbacks,
            )
            # Agar multiple outputs aayen to pehla use kare
            verdicts.append(1 if results[0].verdict else 0)

        # Simple utilization score = relevant / total
        score = sum(verdicts) / len(verdicts) if verdicts else 0.0
        return score
